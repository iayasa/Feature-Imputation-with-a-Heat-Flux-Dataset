{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es dividir el dataset original en dos partes. \n",
    "<BR>\n",
    "\n",
    "i) Incluye los nulos (10.415) que será donde hagamos las prediciones.\n",
    "<BR>\n",
    "\n",
    "ii) No incluye los nulos (21.229) y será el que utilicemos para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_copia:  (31644, 10)\n",
      "Data con los NUN: (10415, 10)\n",
      "Data sin los NUN:  (21229, 10)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "\n",
    "data_copia = data.copy()\n",
    "\n",
    "# Crear un DataFrame con filas que contienen NaN en 'x_e_out [-]'\n",
    "\n",
    "df_nan = data_copia[data_copia['x_e_out [-]'].isnull()]\n",
    "\n",
    "# Crear un DataFrame sin filas que contienen NaN en 'x_e_out [-]'\n",
    "\n",
    "df_sin_nan = data_copia[~data_copia['x_e_out [-]'].isnull()]\n",
    "\n",
    "print(\"Data_copia: \", data_copia.shape)\n",
    "print(\"Data con los NUN:\", df_nan.shape)\n",
    "print(\"Data sin los NUN: \", df_sin_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de x_e_out [-]: -0.0004528625936219324\n"
     ]
    }
   ],
   "source": [
    "# Calcular la media de la columna \"x_e_out [-]\"\n",
    "media_x_e_out = df_sin_nan['x_e_out [-]'].mean()\n",
    "\n",
    "print(\"Media de x_e_out [-]:\", media_x_e_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21229, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizar la codificación one-hot en la columna \"author\" y \"geometry\". \n",
    "\n",
    "data_encoded = pd.get_dummies(df_sin_nan, columns=['author', \"geometry\"])\n",
    "\n",
    "data_encoded.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probamos el modelo de XG_Boost. \n",
    "## A diferencia del modelo anterior en este caso hemos realizado la búsqueda de hiperparámetros mediante Grid_Search_CV y L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.07489417133559047\n",
      "R-squared: 0.4477888215322451\n",
      "Mean Squared Error (MSE): 0.005609136900044781\n",
      "Mean Absolute Error (MAE): 0.05222303180641102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Reemplazar los valores faltantes con la media de la columna correspondiente\n",
    "data_encoded.fillna(data_encoded.mean(), inplace=True)\n",
    "\n",
    "\n",
    "# Separar las características (X) y la variable objetivo (y)\n",
    "X = data_encoded.drop('x_e_out [-]', axis=1)\n",
    "y = data_encoded['x_e_out [-]']\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear un objeto Scaler y ajustarlo a los datos de entrenamiento\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Crear y entrenar el modelo XGBoost con regularización L1\n",
    "modelo_XGB = XGBRegressor(learning_rate=0.1, max_depth=5, n_estimators=100, alpha=0.1, reg_alpha=1)\n",
    "modelo_XGB.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = modelo_XGB.predict(X_test_scaled)\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Calcular el R cuadrado\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calcular el MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ponemos en el mismo formato el dataset NAN y hacemos la predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la codificación one-hot en la columna \"author\" y \"geometry\"\n",
    "\n",
    "data_encoded_NUN = pd.get_dummies(df_nan, columns=['author', \"geometry\"])\n",
    "\n",
    "# Reemplazar los valores faltantes con la media de la columna correspondiente\n",
    "data_encoded_NUN.fillna(data_encoded_NUN.mean(), inplace=True)\n",
    "\n",
    "# Separar las características (X_new) del nuevo DataFrame\n",
    "X_new = data_encoded_NUN.drop('x_e_out [-]', axis=1)\n",
    "\n",
    "# Escalar los datos del nuevo DataFrame utilizando el mismo objeto Scaler ajustado previamente\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Realizar la predicción en el nuevo DataFrame\n",
    "y_new_pred = modelo_XGB.predict(X_new_scaled)\n",
    "\n",
    "# Agregar los valores predichos al nuevo DataFrame\n",
    "data_encoded_NUN['x_e_out [-]'] = y_new_pred\n",
    "\n",
    "# Imprimir el nuevo DataFrame con los valores predichos\n",
    "\n",
    "formato_subm = data_encoded_NUN[['id', 'x_e_out [-]']]\n",
    "\n",
    "# Convertir el DataFrame seleccionado a CSV\n",
    "formato_subm.to_csv('Submission_9.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
